<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/website/libs/katex/katex.min.css"> <link rel=stylesheet  href="/website/css/franklin.css"> <link rel=stylesheet  href="/website/css/poole_hyde.css"> <link rel=stylesheet  href="/website/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/website/assets/favicon.png"> <title>Dataflowr - Deep Learning DIY</title> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/website/assets/dataflowr_violet_plain_square.png" style="width: 120px; height: auto; display: inline"> <img src="/website/assets/favicon.png" style="margin-left:1em; position:relative;left:0px; top:-30px; width: 60px; height: auto; display: inline"> <h1 style="font-size:1em; opacity: 0.95;"><a href="/website/">Deep Learning DIY</a></h1> </div> <nav class=sidebar-nav > <a class="sidebar-nav-item " href="/website/modules/0-sotfware-installation"> <b>Module 0</b> - <em> Software installation</em> </a> <a class="sidebar-nav-item " href="/website/modules/1-intro-general-overview"> <b>Module 1</b> - <em>Introduction & General Overview</em> </a> <a class="sidebar-nav-item " href="/website/modules/2a-pytorch-tensors"> <b>Module 2a</b> - <em>PyTorch tensors</em> </a> <a class="sidebar-nav-item " href="/website/modules/2b-automatic-differentiation"> <b>Module 2b</b> - <em>Automatic differentiation</em> </a> <a class="sidebar-nav-item " href="/website/modules/2c-jax"> <b>Module 2c</b> - <em>Automatic differentiation: VJP and intro to JAX</em> </a> <a class="sidebar-nav-item " href="/website/modules/3-loss-functions-for-classification"> <b>Module 3</b> - <em>Loss functions for classification</em> </a> <a class="sidebar-nav-item " href="/website/modules/4-optimization-for-deep-learning"> <b>Module 4</b> - <em>Optimization for DL</em> </a> <a class="sidebar-nav-item " href="/website/modules/5-stacking-layers"> <b>Module 5</b> - <em>Stacking layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/6-convolutional-neural-network"> <b>Module 6</b> - <em>Convolutional neural network</em> </a> <a class="sidebar-nav-item " href="/website/modules/7-dataloading"> <b>Module 7</b> - <em>Dataloading</em> </a> <a class="sidebar-nav-item " href="/website/modules/8a-embedding-layers"> <b>Module 8a</b> - <em>Embedding layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/8b-collaborative-filtering"> <b>Module 8b</b> - <em>Collaborative filtering</em> </a> <a class="sidebar-nav-item " href="/website/modules/8c-word2vec"> <b>Module 8c</b> - <em>Word2vec</em> </a> <a class="sidebar-nav-item " href="/website/modules/9a-autoencoders"> <b>Module 9a</b> - <em>Autoencoders</em> </a> <a class="sidebar-nav-item " href="/website/modules/9b-unet"> <b>Module 9b</b> - <em>UNets</em> </a> <a class="sidebar-nav-item active" href="/website/modules/9c-flows"> <b>Module 9c</b> - <em>Flows</em> </a> <a class="sidebar-nav-item " href="/website/modules/10-generative-adversarial-networks"> <b>Module 10</b> - <em>Generative adversarial networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/11a-recurrent-neural-networks-theory"> <b>Module 11a</b> - <em>Recurrent Neural Networks (theory)</em> </a> <a class="sidebar-nav-item " href="/website/modules/11b-recurrent-neural-networks-practice"> <b>Module 11b</b> - <em>RNN in practice</em> </a> <a class="sidebar-nav-item " href="/website/modules/11c-batches-with-sequences"> <b>Module 11c</b> - <em>Batches with sequences in Pytorch</em> </a> <a class="sidebar-nav-item " href="/website/modules/12-attention"> <b>Module 12</b> - <em>Attention and Transformers</em> </a> <a class="sidebar-nav-item " href="/website/modules/13-siamese"> <b>Module 13</b> - <em>Siamese Networks and Representation Learning</em> </a> <a class="sidebar-nav-item " href="/website/modules/14a-depth"> <b>Module 14a</b> - <em>The Benefits of Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/14b-depth"> <b>Module 14b</b> - <em>The Problems with Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/15-dropout"> <b>Module 15</b> - <em>Dropout</em> </a> <a class="sidebar-nav-item " href="/website/modules/16-batchnorm"> <b>Module 16</b> - <em>Batchnorm</em> </a> <a class="sidebar-nav-item " href="/website/modules/17-resnets"> <b>Module 17</b> - <em>Resnets</em> </a> <a class="sidebar-nav-item " href="/website/modules/18a-diffusion"> <b>Module 18a</b> - <em>Denoising Diffusion Probabilistic Models</em> </a> <!-- <div class=week >Unit 7</div>--> <div class=week >Homeworks</div> <a class="sidebar-nav-item " href="/website/homework/1-mlp-from-scratch"> <b>Homework 1</b> - <em>MLP from scratch</em> </a> <a class="sidebar-nav-item " href="/website/homework/2-CAM-adversarial"> <b>Homework 2</b> - <em>Class Activation Map and adversarial examples</em> </a> <a class="sidebar-nav-item " href="/website/homework/3-VAE"> <b>Homework 3</b> - <em>VAE for MNIST clustering and generation</em> </a> <div class=week >Bonus</div> <a class="sidebar-nav-item " href="/website/modules/12-intro-julia"> <b>Module</b> - <em>Intro to Julia: Autodiff with dual numbers</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph0"> <b>Module</b> - <em>Deep learning on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph1"> <b>Graph</b> - <em>Node embeddings</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph2"> <b>Graph</b> - <em>Signal processing on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph3"> <b>Graph</b> - <em> Graph embeddings and GNNs</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/GCN_inductivebias_spectral"> <b>Post</b> - <em>Spectral GCN</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/Convolutions_first"> <b>Post</b> - <em>Convolutions from first principles</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/invariant_equivariant"> <b>Post</b> - <em>Invariant and equivariant networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/graph_invariant"> <b>Graph</b> - <em>Exploiting Graph Invariants in Deep Learning</em> </a> <div class=week >Guest Lectures</div> <a class="sidebar-nav-item " href="/website/modules/privacy-preserving-ML"> <b>Privacy Preserving ML</b> - <em>Daniel Huynh</em> </a> </nav> </div> </div> <div class="content container"> <div class=franklin-content ><h1 id=module_9c_-_flows ><a href="#module_9c_-_flows" class=header-anchor >Module 9c - Flows</a></h1> <p><img src="../extras/flows/Real_NVP.png" alt="" /></p> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#density_estimation_using_real_nvp">Density estimation using Real NVP</a><li><a href="#implementation_of_real_nvp">Implementation of Real NVP</a></ol></div> <h1 id=normalizing_flows ><a href="#normalizing_flows" class=header-anchor >Normalizing flows</a></h1> <p>The image below is taken from this very good blog post on normalizing flows: <a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">blogpost</a></p> <p><img src="../extras/flows/three-generative-models.png" alt="" /></p> <p>Here we only describe flow-based generative models, you can have look at <a href="/website/homework/3-VAE">VAE</a> and <a href="/website/modules/10-generative-adversarial-networks">GAN</a>.</p> <p>A <strong>flow-based generative model</strong> is constructed by a sequence of <strong>invertible</strong> transformations. The main advantage of flows is that the model explicitly learns the data distribution \(p(\mathbf{x})\) and therefore the loss function is simply the negative log-likelihood.</p> <p>Given a sample \(\mathbf{x}\) and a prior \(p(\mathbf{z})\), we compute \(f(\mathbf{x}) = \mathbf{z}\) with an invertible function \(f\) that will be learned. Given \(f\) and the prior \(p(\mathbf{z})\), we can compute the evidence \(p(\mathbf{x})\) thanks to the change of variable formula:</p> <div class=nonumber >\[\begin{aligned} \mathbf{z} &\sim p(\mathbf{z}), \mathbf{z} = f(\mathbf{x}), \\ p(\mathbf{x}) &= p(\mathbf{z}) \left\vert \det \dfrac{d \mathbf{z}}{d \mathbf{x}} \right\vert = p(f(\mathbf{x})) \left\vert \det \dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}} \right\vert \end{aligned}\]</div> <p>where \(\dfrac{\partial f(\mathbf{x})}{\partial \mathbf{x}}\) is the Jacobian matrix of \(f\). Recall that given a function mapping a \(n\)-dimensional input vector \(\mathbf{x}\) to a \(m\)-dimensional output vector, \(f: \mathbb{R}^n \mapsto \mathbb{R}^m\), the matrix of all first-order partial derivatives of this function is called the <strong>Jacobian matrix</strong>, \(J_f\) where one entry on the i-th row and j-th column is \((J_f(\mathbf{x}))_{ij} = \frac{\partial f_i(\mathbf{x})}{\partial x_j}\):</p> <div class=nonumber >\[\begin{aligned} {J_f(\mathbf{x})} = \begin{bmatrix} \frac{\partial f_1(\mathbf{x})}{\partial x_1} & \dots & \frac{\partial f_1(\mathbf{x})}{\partial x_n} \\[6pt] \vdots & \ddots & \vdots \\[6pt] \frac{\partial f_m(\mathbf{x})}{\partial x_1} & \dots & \frac{\partial f_m(\mathbf{x})}{\partial x_n} \\[6pt] \end{bmatrix} \end{aligned}\]</div> <p>Below, we will parametrize \(f\) with a neural network and learn \(f\) by maximizing \(\ln p(\mathbf{x})\). More precisely, given a dataset \((\mathbf{x}_1,\dots,\mathbf{x}_n)\) and a model provided by a prior \(p(\mathbf{z})\) and a neural network \(f\), we optimize the weights of \(f\) by minimizing:</p> <div class=nonumber >\[\begin{aligned} -\sum_{i}\ln p(\mathbf{x_i}) = \sum_i -\ln p(f(\mathbf{x}_i)) -\ln\left\vert \det \dfrac{\partial f(\mathbf{x}_i)}{\partial \mathbf{x}} \right\vert. \end{aligned}\]</div> <p><strong>We need to ensure that \(f\) is always invertible and that the determinant is simple to compute.</strong></p> <h2 id=density_estimation_using_real_nvp ><a href="#density_estimation_using_real_nvp" class=header-anchor >Density estimation using Real NVP</a></h2> <p><a href="https://arxiv.org/abs/1605.08803">Real NVP</a> &#40;introduced by Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio in 2016&#41; uses function \(f\) obtained by stacking affine coupling layers which for an input \(\mathbf{x}\in \mathbb{R}^D\) produce the output \(\mathbf{y}\in\mathbb{R}^D\) defined by &#40;with \( d < D \) &#41;: </p> <a id=eqaff  class=anchor ></a>\[\begin{aligned} \mathbf{y}_{1:d} &= \mathbf{x}_{1:d}\\ \mathbf{y}_{d+1:D} &= \mathbf{x}_{d+1:D} \odot \exp\left(s(\mathbf{x}_{1:d})\right) +t(\mathbf{x}_{1:d}) , \end{aligned}\] <p>where \(s\) &#40;scale&#41; and \(t\) &#40;translation&#41; are neural networks mapping \(\mathbb{R}^d\) to \(\mathbb{R}^{D-d}\) and \(\odot\) is the element-wise product.</p> <p>For any functions \(s\) and \(t\), the affine coupling layer is invertible:</p> <div class=nonumber >\[\begin{aligned} \begin{cases} \mathbf{y}_{1:d} &= \mathbf{x}_{1:d} \\ \mathbf{y}_{d+1:D} &= \mathbf{x}_{d+1:D} \odot \exp({s(\mathbf{x}_{1:d})}) + t(\mathbf{x}_{1:d}) \end{cases} \Leftrightarrow \begin{cases} \mathbf{x}_{1:d} &= \mathbf{y}_{1:d} \\ \mathbf{x}_{d+1:D} &= (\mathbf{y}_{d+1:D} - t(\mathbf{y}_{1:d})) \odot \exp(-s(\mathbf{y}_{1:d})) \end{cases} \end{aligned}\]</div> <p>The Jacobian of an affine coupling layer is a lower triangular matrix:</p> <div class=nonumber >\[\begin{aligned} J(\mathbf{x}) = \frac{\partial \mathbf{y}}{\partial \mathbf{x}}= \begin{bmatrix} \mathbb{I}_d & \mathbf{0}_{d\times(D-d)} \\[5pt] \frac{\partial \mathbf{y}_{d+1:D}}{\partial \mathbf{x}_{1:d}} & \text{diag}(\exp(s(\mathbf{x}_{1:d}))) \end{bmatrix} \end{aligned}\]</div> <p>Hence the determinant is simply the product of terms on the diagonal:</p> <div class=nonumber >\[\begin{aligned} \left\vert\det(J(\mathbf{x}))\right\vert = \prod_{j=1}^{D-d}\exp(s(\mathbf{x}_{1:d}))_j = \exp\left(\sum_{j=1}^{D-d} s(\mathbf{x}_{1:d})_j\right) \end{aligned}\]</div> <p>Note that, we do not need to compute the Jacobian of \(s\) or \(t\) and to compute \(f^{-1}\), we do not need to compute the inverse of \(s\) or \(t\) &#40;which might not exist&#33;&#41;. In other words, we can take arbitrary complex functions for \(s\) and \(t\).</p> <p>In one affine coupling layer, some dimensions &#40;channels&#41; remain unchanged. To make sure all the inputs have a chance to be altered, the model reverses the ordering in each layer so that different components are left unchanged. Following such an alternating pattern, the set of units which remain identical in one transformation layer are always modified in the next. </p> <p>This can be implemented with binary masks. First, we can extend the scale and neural networks to mappings form \(\mathbb{R}^D\) to \(\mathbb{R}^D\). Then taking a mask \(\mathbf{b} = (1,\dots,1,0,\dots,0)\) with \(d\) ones, so that we have for the affine layer:</p> <div class=nonumber >\[\begin{aligned} \mathbf{y} = \mathbf{x} \odot \exp\big((1-\mathbf{b}) \odot s(\mathbf{b} \odot \mathbf{x})\big) + (1-\mathbf{b}) \odot t(\mathbf{b} \odot \mathbf{x}). \end{aligned}\]</div> <p>Note that we have</p> <div class=nonumber >\[\begin{aligned} \ln \left\vert\det(J(\mathbf{x}))\right\vert = \sum_{j=1}^{D} \Big((1-\mathbf{b})\odot s(\mathbf{b} \odot \mathbf{x})\Big)_j, \end{aligned}\]</div> <p>and to invert the affine layer:</p> <div class=nonumber >\[\begin{aligned} \mathbf{x} = \left( \mathbf{y} -(1-\mathbf{b}) \odot t(\mathbf{b} \odot \mathbf{y})\right)\odot \exp\left( -(1-\mathbf{b}) \odot s(\mathbf{b} \odot \mathbf{y})\right). \end{aligned}\]</div> <p>Now we alternates the binary mask \(\mathbf{b}\) from one coupling layer to the other. </p> <p>Note, that the formula given in the paper is slightly different:</p> \[\mathbf{y} = \mathbf{b} \odot \mathbf{x} + (1 - \mathbf{b}) \odot \Big(\mathbf{x} \odot \exp\big(s(\mathbf{b} \odot \mathbf{x})\big) + t(\mathbf{b} \odot \mathbf{x})\Big),\] <p>but the 2 formulas give the same result&#33;</p> <h2 id=implementation_of_real_nvp ><a href="#implementation_of_real_nvp" class=header-anchor >Implementation of Real NVP</a></h2> <ul> <li><p>you can now implement your <a href="https://github.com/dataflowr/notebooks/blob/master/Module9/Normalizing_flows_empty.ipynb">own NVP</a></p> <li><p>and here is the <a href="https://github.com/dataflowr/notebooks/blob/master/Module9/Normalizing_flows_sol.ipynb">solution</a></p> </ul> <div class=page-foot > <div class=copyright > <a href="https://github.com/dataflowr/website/tree/master"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> Last modified: April 25, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> <script src="/website/libs/katex/katex.min.js"></script> <script src="/website/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>