<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/website/libs/katex/katex.min.css"> <link rel=stylesheet  href="/website/libs/highlight/github.min.css"> <link rel=stylesheet  href="/website/css/franklin.css"> <link rel=stylesheet  href="/website/css/poole_hyde.css"> <link rel=stylesheet  href="/website/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/website/assets/favicon.png"> <title>Dataflowr - Deep Learning DIY</title> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/website/assets/dataflowr_violet_plain_square.png" style="width: 120px; height: auto; display: inline"> <img src="/website/assets/favicon.png" style="margin-left:1em; position:relative;left:0px; top:-30px; width: 60px; height: auto; display: inline"> <h1 style="font-size:1em; opacity: 0.95;"><a href="/website/">Deep Learning DIY</a></h1> </div> <nav class=sidebar-nav > <a class="sidebar-nav-item " href="/website/modules/0-sotfware-installation"> <b>Module 0</b> - <em> Software installation</em> </a> <a class="sidebar-nav-item " href="/website/modules/1-intro-general-overview"> <b>Module 1</b> - <em>Introduction & General Overview</em> </a> <a class="sidebar-nav-item " href="/website/modules/2a-pytorch-tensors"> <b>Module 2a</b> - <em>PyTorch tensors</em> </a> <a class="sidebar-nav-item " href="/website/modules/2b-automatic-differentiation"> <b>Module 2b</b> - <em>Automatic differentiation</em> </a> <a class="sidebar-nav-item " href="/website/modules/2c-jax"> <b>Module 2c</b> - <em>Automatic differentiation: VJP and intro to JAX</em> </a> <a class="sidebar-nav-item " href="/website/modules/3-loss-functions-for-classification"> <b>Module 3</b> - <em>Loss functions for classification</em> </a> <a class="sidebar-nav-item " href="/website/modules/4-optimization-for-deep-learning"> <b>Module 4</b> - <em>Optimization for DL</em> </a> <a class="sidebar-nav-item " href="/website/modules/5-stacking-layers"> <b>Module 5</b> - <em>Stacking layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/6-convolutional-neural-network"> <b>Module 6</b> - <em>Convolutional neural network</em> </a> <a class="sidebar-nav-item " href="/website/modules/7-dataloading"> <b>Module 7</b> - <em>Dataloading</em> </a> <a class="sidebar-nav-item " href="/website/modules/8a-embedding-layers"> <b>Module 8a</b> - <em>Embedding layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/8b-collaborative-filtering"> <b>Module 8b</b> - <em>Collaborative filtering</em> </a> <a class="sidebar-nav-item " href="/website/modules/8c-word2vec"> <b>Module 8c</b> - <em>Word2vec</em> </a> <a class="sidebar-nav-item " href="/website/modules/9a-autoencoders"> <b>Module 9a</b> - <em>Autoencoders</em> </a> <a class="sidebar-nav-item " href="/website/modules/9b-unet"> <b>Module 9b</b> - <em>UNets</em> </a> <a class="sidebar-nav-item " href="/website/modules/9c-flows"> <b>Module 9c</b> - <em>Flows</em> </a> <a class="sidebar-nav-item " href="/website/modules/10-generative-adversarial-networks"> <b>Module 10</b> - <em>Generative adversarial networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/11a-recurrent-neural-networks-theory"> <b>Module 11a</b> - <em>Recurrent Neural Networks (theory)</em> </a> <a class="sidebar-nav-item " href="/website/modules/11b-recurrent-neural-networks-practice"> <b>Module 11b</b> - <em>RNN in practice</em> </a> <a class="sidebar-nav-item " href="/website/modules/11c-batches-with-sequences"> <b>Module 11c</b> - <em>Batches with sequences in Pytorch</em> </a> <a class="sidebar-nav-item " href="/website/modules/12-attention"> <b>Module 12</b> - <em>Attention and Transformers</em> </a> <a class="sidebar-nav-item " href="/website/modules/13-siamese"> <b>Module 13</b> - <em>Siamese Networks and Representation Learning</em> </a> <a class="sidebar-nav-item " href="/website/modules/14a-depth"> <b>Module 14a</b> - <em>The Benefits of Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/14b-depth"> <b>Module 14b</b> - <em>The Problems with Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/15-dropout"> <b>Module 15</b> - <em>Dropout</em> </a> <a class="sidebar-nav-item " href="/website/modules/16-batchnorm"> <b>Module 16</b> - <em>Batchnorm</em> </a> <a class="sidebar-nav-item " href="/website/modules/17-resnets"> <b>Module 17</b> - <em>Resnets</em> </a> <a class="sidebar-nav-item " href="/website/modules/18a-diffusion"> <b>Module 18a</b> - <em>Denoising Diffusion Probabilistic Models</em> </a> <!-- <div class=week >Unit 7</div>--> <div class=week >Homeworks</div> <a class="sidebar-nav-item " href="/website/homework/1-mlp-from-scratch"> <b>Homework 1</b> - <em>MLP from scratch</em> </a> <a class="sidebar-nav-item " href="/website/homework/2-CAM-adversarial"> <b>Homework 2</b> - <em>Class Activation Map and adversarial examples</em> </a> <a class="sidebar-nav-item " href="/website/homework/3-VAE"> <b>Homework 3</b> - <em>VAE for MNIST clustering and generation</em> </a> <div class=week >Bonus</div> <a class="sidebar-nav-item " href="/website/modules/12-intro-julia"> <b>Module</b> - <em>Intro to Julia: Autodiff with dual numbers</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph0"> <b>Module</b> - <em>Deep learning on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph1"> <b>Graph</b> - <em>Node embeddings</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph2"> <b>Graph</b> - <em>Signal processing on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph3"> <b>Graph</b> - <em> Graph embeddings and GNNs</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/GCN_inductivebias_spectral"> <b>Post</b> - <em>Spectral GCN</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/Convolutions_first"> <b>Post</b> - <em>Convolutions from first principles</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/invariant_equivariant"> <b>Post</b> - <em>Invariant and equivariant networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/graph_invariant"> <b>Graph</b> - <em>Exploiting Graph Invariants in Deep Learning</em> </a> <div class=week >Guest Lectures</div> <a class="sidebar-nav-item " href="/website/modules/privacy-preserving-ML"> <b>Privacy Preserving ML</b> - <em>Daniel Huynh</em> </a> </nav> </div> </div> <div class="content container"> <div class=franklin-content ><p><a href="https://dataflowr.github.io/website/"><img src="https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png" alt=Dataflowr  /></a></p> <p>You are viewing the static version of the notebook, you can get the <a href="https://github.com/dataflowr/notebooks/blob/master/Module2/02a_basics.ipynb">code &#40;GitHub&#41;</a> or run it in <a href="https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module2/02a_basics.ipynb">colab</a></p> <p>You can also do the <a href="https://dataflowr.github.io/quiz/module2a.html">quizzes</a></p> <h1 id=module_2_pytorch_tensors_and_automatic_differentiation ><a href="#module_2_pytorch_tensors_and_automatic_differentiation" class=header-anchor >Module 2: PyTorch tensors and automatic differentiation</a></h1> <p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;103">Video timestamp</a></p> <pre><code class=language-python >import matplotlib.pyplot as plt
&#37;matplotlib inline
import torch
import numpy as np</code></pre> <pre><code class=language-python >torch.__version__</code></pre>
<p>Tensors are used to encode the signal to process, but also the internal states and parameters of models.</p>
<p><strong>Manipulating data through this constrained structure allows to use CPUs and GPUs at peak performance.</strong></p>
<p>Construct a 3x5 matrix, uninitialized:</p>
<pre><code class=language-python >x &#61; torch.empty&#40;3,5&#41;
print&#40;x.dtype&#41;
print&#40;x&#41;</code></pre>
<p>If you got an error this <a href="https://stackoverflow.com/questions/50617917/overflow-when-unpacking-long-pytorch">stackoverflow link</a> might be useful...</p>
<pre><code class=language-python >x &#61; torch.randn&#40;3,5&#41;
print&#40;x&#41;</code></pre>
<pre><code class=language-python >print&#40;x.size&#40;&#41;&#41;</code></pre>
<p>torch.Size is in fact a <a href="https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences">tuple</a>, so it supports the same operations.</p>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;272">Video timestamp</a></p>
<pre><code class=language-python >x.size&#40;&#41;&#91;1&#93;</code></pre>
<pre><code class=language-python >x.size&#40;&#41; &#61;&#61; &#40;3,5&#41;</code></pre>
<h3 id=bridge_to_numpy ><a href="#bridge_to_numpy" class=header-anchor >Bridge to numpy</a></h3>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;325">Video timestamp</a></p>
<pre><code class=language-python >y &#61; x.numpy&#40;&#41;
print&#40;y&#41;</code></pre>
<pre><code class=language-python >a &#61; np.ones&#40;5&#41;
b &#61; torch.from_numpy&#40;a&#41;
print&#40;a.dtype&#41;
print&#40;b&#41;</code></pre>
<pre><code class=language-python >c &#61; b.long&#40;&#41;
print&#40;c.dtype, c&#41;
print&#40;b.dtype, b&#41;</code></pre>
<pre><code class=language-python >xr &#61; torch.randn&#40;3, 5&#41;
print&#40;xr.dtype, xr&#41;</code></pre>
<pre><code class=language-python >resb &#61; xr &#43; b
resb</code></pre>
<pre><code class=language-python >resc &#61; xr &#43; c
resc</code></pre>
<p>Be careful with types&#33;</p>
<pre><code class=language-python >resb &#61;&#61; resc</code></pre>
<pre><code class=language-python >torch.set_printoptions&#40;precision&#61;10&#41;</code></pre>
<pre><code class=language-python >resb&#91;0,1&#93;</code></pre>
<pre><code class=language-python >resc&#91;0,1&#93;</code></pre>
<pre><code class=language-python >resc&#91;0,1&#93;.dtype</code></pre>
<pre><code class=language-python >xr&#91;0,1&#93;</code></pre>
<pre><code class=language-python >torch.set_printoptions&#40;precision&#61;4&#41;</code></pre>
<h3 id=a_hrefhttpsdocsscipyorgdocnumpy-1130userbasicsbroadcastinghtmlbroadcasting ><a href="#a_hrefhttpsdocsscipyorgdocnumpy-1130userbasicsbroadcastinghtmlbroadcasting" class=header-anchor ><a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">Broadcasting</a></a></h3>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;670">Video timestamp</a></p>
<p>Broadcasting automagically expands dimensions by replicating coefficients, when it is necessary to perform operations.</p>
<ol>
<li><p>If one of the tensors has fewer dimensions than the other, it is reshaped by adding as many dimensions of size 1 as necessary in the front; then</p>

<li><p>for every mismatch, if one of the two tensor is of size one, it is expanded along this axis by replicating  coefficients.</p>

</ol>
<p>If there is a tensor size mismatch for one of the dimension and neither of them is one, the operation fails.</p>
<pre><code class=language-python >A &#61; torch.tensor&#40;&#91;&#91;1.&#93;, &#91;2.&#93;, &#91;3.&#93;, &#91;4.&#93;&#93;&#41;
print&#40;A.size&#40;&#41;&#41;
B &#61; torch.tensor&#40;&#91;&#91;5., -5., 5., -5., 5.&#93;&#93;&#41;
print&#40;B.size&#40;&#41;&#41;
C &#61; A &#43; B</code></pre>
<pre><code class=language-python >C</code></pre>
<p>The original &#40;column-&#41;vector</p>
\[\begin{array}{rcl}
A = \left( \begin{array}{c}
1\\
2\\
3\\
4\\
\end{array}\right)
\end{array}\]
<p>is transformed into the matrix </p>
\[\begin{array}{rcl}
A = \left( \begin{array}{ccccc}
1&1&1&1&1\\
2&2&2&2&2\\
3&3&3&3&3\\
4&4&4&4&4
\end{array}\right)
\end{array}\]
<p>and the original &#40;row-&#41;vector</p>
\[\begin{array}{rcl}
C = (5,-5,5,-5,5)
\end{array}\]
<p>is transformed into the matrix</p>
\[\begin{array}{rcl}
C = \left( \begin{array}{ccccc}
5&-5&5&-5&5\\
5&-5&5&-5&5\\
5&-5&5&-5&5\\
5&-5&5&-5&5
\end{array}\right)
\end{array}\]
<p>so that summing these matrices gives:</p>
\[\begin{array}{rcl}
A+C = \left( \begin{array}{ccccc}
6&-4&6&-4&6\\
7&-3&7&-3&7\\
8&-2&8&-2&8\\
9&-1&9&-1&9
\end{array}\right)
\end{array}\]
<h3 id=in-place_modification ><a href="#in-place_modification" class=header-anchor >In-place modification</a></h3>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;875">Video timestamp</a></p>
<pre><code class=language-python >x</code></pre>
<pre><code class=language-python >xr</code></pre>
<pre><code class=language-python >print&#40;x&#43;xr&#41;</code></pre>
<pre><code class=language-python >x.add_&#40;xr&#41;
print&#40;x&#41;</code></pre>
<p>Any operation that mutates a tensor in-place is post-fixed with an <code>_</code></p>
<p>For example: <code>x.fill_&#40;y&#41;</code>, <code>x.t_&#40;&#41;</code>, will change <code>x</code>.</p>
<pre><code class=language-python >print&#40;x.t&#40;&#41;&#41;</code></pre>
<pre><code class=language-python >x.t_&#40;&#41;
print&#40;x&#41;</code></pre>
<h3 id=shared_memory ><a href="#shared_memory" class=header-anchor >Shared memory</a></h3>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;990">Video timestamp</a></p>
<p>Also be careful, changing the torch tensor modify the numpy array and vice-versa...</p>
<p>This is explained in the PyTorch documentation <a href="https://pytorch.org/docs/stable/torch.html#torch.from_numpy">here</a>: The returned tensor by <code>torch.from_numpy</code> and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. </p>
<pre><code class=language-python >a &#61; np.ones&#40;5&#41;
b &#61; torch.from_numpy&#40;a&#41;
print&#40;b&#41;</code></pre>
<pre><code class=language-python >a&#91;2&#93; &#61; 0
print&#40;b&#41;</code></pre>
<pre><code class=language-python >b&#91;3&#93; &#61; 5
print&#40;a&#41;</code></pre>
<h3 id=cuda ><a href="#cuda" class=header-anchor >Cuda</a></h3>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;1120">Video timestamp</a></p>
<pre><code class=language-python >torch.cuda.is_available&#40;&#41;</code></pre>
<pre><code class=language-python >#device &#61; torch.device&#40;&#39;cpu&#39;&#41;
device &#61; torch.device&#40;&#39;cuda&#39;&#41; # Uncomment this to run on GPU</code></pre>
<pre><code class=language-python >x.device</code></pre>
<pre><code class=language-python ># let us run this cell only if CUDA is available
# We will use &#96;&#96;torch.device&#96;&#96; objects to move tensors in and out of GPU
if torch.cuda.is_available&#40;&#41;:
    y &#61; torch.ones_like&#40;x, device&#61;device&#41;  # directly create a tensor on GPU
    x &#61; x.to&#40;device&#41;                       # or just use strings &#96;&#96;.to&#40;&quot;cuda&quot;&#41;&#96;&#96;
    z &#61; x &#43; y
    print&#40;z,z.type&#40;&#41;&#41;
    print&#40;z.to&#40;&quot;cpu&quot;, torch.double&#41;&#41;       # &#96;&#96;.to&#96;&#96; can also change dtype together&#33;</code></pre>
<pre><code class=language-python >x &#61; torch.randn&#40;1&#41;
x &#61; x.to&#40;device&#41;</code></pre>
<pre><code class=language-python >x.device</code></pre>
<pre><code class=language-python ># the following line is only useful if CUDA is available
x &#61; x.data
print&#40;x&#41;
print&#40;x.item&#40;&#41;&#41;
print&#40;x.cpu&#40;&#41;.numpy&#40;&#41;&#41;</code></pre>
<h1 id=simple_interfaces_to_standard_image_data-bases ><a href="#simple_interfaces_to_standard_image_data-bases" class=header-anchor >Simple interfaces to standard image data-bases</a></h1>
<p><a href="https://youtu.be/BmAS8IH7n3c?t&#61;1354">Video timestamp</a></p>
<p>An example, the <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.CIFAR10">CIFAR10</a> dataset.</p>
<pre><code class=language-python >import torchvision

data_dir &#61; &#39;content/data&#39;

cifar &#61; torchvision.datasets.CIFAR10&#40;data_dir, train &#61; True, download &#61; True&#41;
cifar.data.shape</code></pre>
<p>Documentation about the <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"><code>permute</code></a> operation.</p>
<pre><code class=language-python >x &#61; torch.from_numpy&#40;cifar.data&#41;.permute&#40;0,3,1,2&#41;.float&#40;&#41;
x &#61; x / 255
print&#40;x.type&#40;&#41;, x.size&#40;&#41;, x.min&#40;&#41;.item&#40;&#41;, x.max&#40;&#41;.item&#40;&#41;&#41;</code></pre>
<p>Documentation about the <a href="https://pytorch.org/docs/stable/torch.html#torch.narrow"><code>narrow&#40;input, dim, start, length&#41;</code></a> operation.</p>
<pre><code class=language-python ># Narrows to the first images, converts to float
x &#61; torch.narrow&#40;x, 0, 0, 48&#41;</code></pre>
<pre><code class=language-python >x.shape</code></pre>
<pre><code class=language-python ># Showing images
def show&#40;img&#41;:
    npimg &#61; img.numpy&#40;&#41;
    plt.figure&#40;figsize&#61;&#40;20,10&#41;&#41;
    plt.imshow&#40;np.transpose&#40;npimg, &#40;1,2,0&#41;&#41;, interpolation&#61;&#39;nearest&#39;&#41;
    
show&#40;torchvision.utils.make_grid&#40;x, nrow &#61; 12&#41;&#41;</code></pre>
<pre><code class=language-python ># Kills the green and blue channels
x.narrow&#40;1, 1, 2&#41;.fill_&#40;0&#41;
show&#40;torchvision.utils.make_grid&#40;x, nrow &#61; 12&#41;&#41;</code></pre>
<h1 id=autograd_automatic_differentiation ><a href="#autograd_automatic_differentiation" class=header-anchor >Autograd: automatic differentiation</a></h1>
<p><a href="https://youtu.be/Z6H3zakmn6E?t&#61;40">Video timestamp</a></p>
<p>When executing tensor operations, PyTorch can automatically construct on-the-fly the graph of operations to compute the gradient of any quantity with respect to any tensor involved.</p>
<p>To be more concrete, we introduce the following example: we consider parameters \(w\in \mathbb{R}\) and \(b\in \mathbb{R}\) with the corresponding function:</p>
<div class=nonumber >\[\begin{array}{rcl}
\ell = \left(\exp(wx+b) - y^* \right)^2
\end{array}\]</div>
<p>Our goal here, will be to compute the following partial derivatives:</p>
<div class=nonumber >\[\begin{array}{rcl}
\frac{\partial \ell}{\partial w}\mbox{ and, }\frac{\partial \ell}{\partial b}.
\end{array}\]</div>
<p>The reason for doing this will be clear when you will solve the practicals for this lesson&#33;</p>
<p>You can decompose this function as a composition of basic operations. This is call the forward pass on the graph of operations. <img src="https://dataflowr.github.io/notebooks/Module2/img/backprop1.png" alt=backprop1  /></p>
<p>Let say we start with our model in <code>numpy</code>:</p>
<pre><code class=language-python >w &#61; np.array&#40;&#91;0.5&#93;&#41;
b &#61; np.array&#40;&#91;2&#93;&#41;
xx &#61; np.array&#40;&#91;0.5&#93;&#41;#np.arange&#40;0,1.5,.5&#41;</code></pre>
<p>transform these into <code>tensor</code>:</p>
<pre><code class=language-python >xx_t &#61; torch.from_numpy&#40;xx&#41;
w_t &#61; torch.from_numpy&#40;w&#41;
b_t &#61; torch.from_numpy&#40;b&#41;</code></pre>
<p><a href="https://youtu.be/Z6H3zakmn6E?t&#61;224">Video timestamp</a></p>
<p>A <code>tensor</code> has a Boolean field <code>requires_grad</code>, set to <code>False</code> by default, which states if PyTorch should build the graph of operations so that gradients with respect to it can be computed.</p>
<pre><code class=language-python >w_t.requires_grad</code></pre>
<p>We want to take derivative with respect to \(w\) so we change this value:</p>
<pre><code class=language-python >w_t.requires_grad_&#40;True&#41;</code></pre>
<p>We want to do the same thing for \(b\) but the following line will produce an error&#33;</p>
<pre><code class=language-python >b_t.requires_grad_&#40;True&#41;</code></pre>
<p>Reading the error message should allow you to correct the mistake&#33;</p>
<pre><code class=language-python >dtype &#61; torch.float64</code></pre>
<pre><code class=language-python >b_t &#61; b_t.type&#40;dtype&#41;</code></pre>
<pre><code class=language-python >b_t.requires_grad_&#40;True&#41;</code></pre>
<p><a href="https://youtu.be/Z6H3zakmn6E?t&#61;404">Video timestamp</a></p>
<p>We now compute the function:</p>
<pre><code class=language-python >def fun&#40;x,ystar&#41;:
    y &#61; torch.exp&#40;w_t*x&#43;b_t&#41;
    print&#40;y&#41;
    return torch.sum&#40;&#40;y-ystar&#41;**2&#41;

ystar_t &#61; torch.randn_like&#40;xx_t&#41;
l_t &#61; fun&#40;xx_t,ystar_t&#41;</code></pre>
<pre><code class=language-python >l_t</code></pre>
<pre><code class=language-python >l_t.requires_grad</code></pre>
<p>After the computation is finished, i.e. <em>forward pass</em>, you can call <code>.backward&#40;&#41;</code> and have all the gradients computed automatically.</p>
<pre><code class=language-python >print&#40;w_t.grad&#41;</code></pre>
<pre><code class=language-python >l_t.backward&#40;&#41;</code></pre>
<pre><code class=language-python >print&#40;w_t.grad&#41;
print&#40;b_t.grad&#41;</code></pre>
<p><a href="https://youtu.be/Z6H3zakmn6E?t&#61;545">Video timestamp</a></p>
<p>Let&#39;s try to understand these numbers...</p>
<p><img src="https://dataflowr.github.io/notebooks/Module2/img/backprop2.png" alt=backprop2  /></p>
<pre><code class=language-python >yy_t &#61; torch.exp&#40;w_t*xx_t&#43;b_t&#41;
print&#40;torch.sum&#40;2*&#40;yy_t-ystar_t&#41;*yy_t*xx_t&#41;&#41;
print&#40;torch.sum&#40;2*&#40;yy_t-ystar_t&#41;*yy_t&#41;&#41;</code></pre>
<p><code>tensor.backward&#40;&#41;</code> accumulates the gradients in  the <code>grad</code> fields  of tensors.</p>
<pre><code class=language-python >l_t &#61; fun&#40;xx_t,ystar_t&#41;
l_t.backward&#40;&#41;</code></pre>
<pre><code class=language-python >print&#40;w_t.grad&#41;
print&#40;b_t.grad&#41;</code></pre>
<p>By default, <code>backward</code> deletes the computational graph when it is used so that you will get an error below:</p>
<pre><code class=language-python >l_t.backward&#40;&#41;</code></pre>
<pre><code class=language-python ># Manually zero the gradients
w_t.grad.data.zero_&#40;&#41;
b_t.grad.data.zero_&#40;&#41;
l_t &#61; fun&#40;xx_t,ystar_t&#41;
l_t.backward&#40;retain_graph&#61;True&#41;
l_t.backward&#40;&#41;
print&#40;w_t.grad&#41;
print&#40;b_t.grad&#41;</code></pre>
<p>The gradients must be set to zero manually. Otherwise they will cumulate across several <em>.backward&#40;&#41;</em> calls.  This accumulating behavior is desirable in particular to compute the gradient of a loss summed over several “mini-batches,” or the gradient of a sum of losses.</p>
<p><a href="https://dataflowr.github.io/website/"><img src="https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png" alt=Dataflowr  /></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/dataflowr/website/tree/master"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a>
    Last modified: April 18, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>  
    
        <script src="/website/libs/katex/katex.min.js"></script>
<script src="/website/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/website/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>