<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/website/css/franklin.css"> <link rel=stylesheet  href="/website/css/poole_hyde.css"> <link rel=stylesheet  href="/website/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/website/assets/favicon.png"> <title>Dataflowr - Deep Learning DIY</title> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/website/assets/dataflowr_violet_plain_square.png" style="width: 120px; height: auto; display: inline"> <img src="/website/assets/favicon.png" style="margin-left:1em; position:relative;left:0px; top:-30px; width: 60px; height: auto; display: inline"> <h1 style="font-size:1em; opacity: 0.95;"><a href="/website/">Deep Learning DIY</a></h1> </div> <nav class=sidebar-nav > <a class="sidebar-nav-item " href="/website/modules/0-sotfware-installation"> <b>Module 0</b> - <em> Software installation</em> </a> <a class="sidebar-nav-item " href="/website/modules/1-intro-general-overview"> <b>Module 1</b> - <em>Introduction & General Overview</em> </a> <a class="sidebar-nav-item " href="/website/modules/2a-pytorch-tensors"> <b>Module 2a</b> - <em>PyTorch tensors</em> </a> <a class="sidebar-nav-item " href="/website/modules/2b-automatic-differentiation"> <b>Module 2b</b> - <em>Automatic differentiation</em> </a> <a class="sidebar-nav-item " href="/website/modules/2c-jax"> <b>Module 2c</b> - <em>Automatic differentiation: VJP and intro to JAX</em> </a> <a class="sidebar-nav-item " href="/website/modules/3-loss-functions-for-classification"> <b>Module 3</b> - <em>Loss functions for classification</em> </a> <a class="sidebar-nav-item " href="/website/modules/4-optimization-for-deep-learning"> <b>Module 4</b> - <em>Optimization for DL</em> </a> <a class="sidebar-nav-item " href="/website/modules/5-stacking-layers"> <b>Module 5</b> - <em>Stacking layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/6-convolutional-neural-network"> <b>Module 6</b> - <em>Convolutional neural network</em> </a> <a class="sidebar-nav-item " href="/website/modules/7-dataloading"> <b>Module 7</b> - <em>Dataloading</em> </a> <a class="sidebar-nav-item " href="/website/modules/8a-embedding-layers"> <b>Module 8a</b> - <em>Embedding layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/8b-collaborative-filtering"> <b>Module 8b</b> - <em>Collaborative filtering</em> </a> <a class="sidebar-nav-item " href="/website/modules/8c-word2vec"> <b>Module 8c</b> - <em>Word2vec</em> </a> <a class="sidebar-nav-item " href="/website/modules/9a-autoencoders"> <b>Module 9a</b> - <em>Autoencoders</em> </a> <a class="sidebar-nav-item " href="/website/modules/9b-unet"> <b>Module 9b</b> - <em>UNets</em> </a> <a class="sidebar-nav-item " href="/website/modules/9c-flows"> <b>Module 9c</b> - <em>Flows</em> </a> <a class="sidebar-nav-item " href="/website/modules/10-generative-adversarial-networks"> <b>Module 10</b> - <em>Generative adversarial networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/11a-recurrent-neural-networks-theory"> <b>Module 11a</b> - <em>Recurrent Neural Networks (theory)</em> </a> <a class="sidebar-nav-item " href="/website/modules/11b-recurrent-neural-networks-practice"> <b>Module 11b</b> - <em>RNN in practice</em> </a> <a class="sidebar-nav-item " href="/website/modules/11c-batches-with-sequences"> <b>Module 11c</b> - <em>Batches with sequences in Pytorch</em> </a> <a class="sidebar-nav-item " href="/website/modules/12-attention"> <b>Module 12</b> - <em>Attention and Transformers</em> </a> <a class="sidebar-nav-item " href="/website/modules/13-siamese"> <b>Module 13</b> - <em>Siamese Networks and Representation Learning</em> </a> <a class="sidebar-nav-item " href="/website/modules/14a-depth"> <b>Module 14a</b> - <em>The Benefits of Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/14b-depth"> <b>Module 14b</b> - <em>The Problems with Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/15-dropout"> <b>Module 15</b> - <em>Dropout</em> </a> <a class="sidebar-nav-item " href="/website/modules/16-batchnorm"> <b>Module 16</b> - <em>Batchnorm</em> </a> <a class="sidebar-nav-item " href="/website/modules/17-resnets"> <b>Module 17</b> - <em>Resnets</em> </a> <a class="sidebar-nav-item " href="/website/modules/18a-diffusion"> <b>Module 18a</b> - <em>Denoising Diffusion Probabilistic Models</em> </a> <!-- <div class=week >Unit 7</div>--> <div class=week >Homeworks</div> <a class="sidebar-nav-item " href="/website/homework/1-mlp-from-scratch"> <b>Homework 1</b> - <em>MLP from scratch</em> </a> <a class="sidebar-nav-item " href="/website/homework/2-CAM-adversarial"> <b>Homework 2</b> - <em>Class Activation Map and adversarial examples</em> </a> <a class="sidebar-nav-item " href="/website/homework/3-VAE"> <b>Homework 3</b> - <em>VAE for MNIST clustering and generation</em> </a> <div class=week >Bonus</div> <a class="sidebar-nav-item " href="/website/modules/12-intro-julia"> <b>Module</b> - <em>Intro to Julia: Autodiff with dual numbers</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph0"> <b>Module</b> - <em>Deep learning on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph1"> <b>Graph</b> - <em>Node embeddings</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph2"> <b>Graph</b> - <em>Signal processing on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph3"> <b>Graph</b> - <em> Graph embeddings and GNNs</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/GCN_inductivebias_spectral"> <b>Post</b> - <em>Spectral GCN</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/Convolutions_first"> <b>Post</b> - <em>Convolutions from first principles</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/invariant_equivariant"> <b>Post</b> - <em>Invariant and equivariant networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/graph_invariant"> <b>Graph</b> - <em>Exploiting Graph Invariants in Deep Learning</em> </a> <div class=week >Guest Lectures</div> <a class="sidebar-nav-item " href="/website/modules/privacy-preserving-ML"> <b>Privacy Preserving ML</b> - <em>Daniel Huynh</em> </a> </nav> </div> </div> <div class="content container"> <div class=franklin-content ><h1 id=deep_learning_do_it_yourself ><a href="#deep_learning_do_it_yourself" class=header-anchor >Deep Learning Do It Yourself&#33;</a></h1> <p style="font-size: 1.15em; color: #333; line-height:1.5em"> This site collects resources to learn Deep Learning in the form of Modules available through the sidebar on the left. As a student, you can walk through the modules at your own pace and interact with others thanks to the associated <a href="https://discord.gg/nZQ3fe3">Discord server</a>. You donâ€™t need any special hardware or software.</p> <h2 id=practical_deep_learning_course ><a href="#practical_deep_learning_course" class=header-anchor >Practical deep learning course</a></h2> <p>The main goal of the course is to allow students to understand papers, blog posts and codes available online and to adapt them to their projects as soon as possible. In particular, we avoid the use of any high-level neural networks API and focus on the <a href="https://pytorch.org/">PyTorch</a> library in Python.</p> <p>The course is divided into sessions &#40;containing possibly several modules&#41;, each session requiring a significant amount of coding. At the end of this course, students were able to read very recent papers and reproduce &#40;or even ameliorate&#41; their experiments. </p> <p>All the code used in this course is available on the GitHub repository <a href="https://github.com/dataflowr/notebooks">dataflowr/notebooks</a>. You will find the solutions to the practicals on this repo&#33; You can fork the repo if you want to run the code locally: <a href="https://docs.github.com/en/get-started/quickstart/fork-a-repo">GitHub Docs about fork</a> then follow the steps in <a href="./modules/0-sotfware-installation/">Module 0</a>. Most of the code will not require a GPU. </p> <p>âš  When a GPU is required , you can launch the code on colab by following the corresponding link given in the module &#40;see for example <a href="./modules/1-intro-general-overview/">Module 1</a>&#41;.</p> <p>Pre-requisites:</p> <ul> <li><p>Mathematics: basics of linear algebra, probability, differential calculus and optimization</p> <li><p>Programming: Python. Test your proficiency: <a href="https://dataflowr.github.io/quiz/python.html">quiz</a></p> </ul> <h3 id=session_1_-_finetuning_vgg ><a href="#session_1_-_finetuning_vgg" class=header-anchor >ðŸŒ» Session 1 - Finetuning VGG</a></h3> <p>Start right away and train a deep neural network on a GPU with <a href="./modules/1-intro-general-overview/">Module 1 - Introduction &amp; General Overview</a></p> <p>Be sure to build your own classifier with more dogs and cats in the practicals. <details> <summary>Things to remember</summary> </p> <blockquote> <ul> <li><p>you do not need to understand everything to run a deep learning model&#33; But the main goal of this course will be to come back to each step done today and understand them...</p> <li><p>to use the dataloader from Pytorch, you need to follow the API &#40;i.e. for classification store your dataset in folders&#41;</p> <li><p>using a pretrained model and modifying it to adapt it to a similar task is easy. </p> <li><p>if you do not understand why we take this loss, that&#39;s fine, we&#39;ll cover that in Module 3.</p> <li><p>even with a GPU, avoid unnecessary computations&#33;</p> </ul> </blockquote> </details> <h3 id=session_2_-_pytorch_tensors_and_autodiff ><a href="#session_2_-_pytorch_tensors_and_autodiff" class=header-anchor >ðŸŒ» Session 2 - PyTorch tensors and Autodiff</a></h3> <ul> <li><p><a href="https://dataflowr.github.io/website/modules/2a-pytorch-tensors/">Module 2a - PyTorch tensors</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/2b-automatic-differentiation/">Module 2b - Automatic differentiation</a> &#43; Practicals</p> <li><p>MLP from scratch start of <a href="https://dataflowr.github.io/website/homework/1-mlp-from-scratch/">HW1</a> </p> <li><p><a href="https://github.com/dataflowr/notebooks/blob/master/Module2/AD_with_dual_numbers_Julia.ipynb">another look at autodiff with dual numbers and Julia</a></p> </ul> <details> <summary>Things to remember</summary> <blockquote> <ul> <li><p>Pytorch tensors &#61; Numpy on GPU &#43; gradients&#33;</p> <li><p>in deep learning, <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting</a> is used everywhere. The rules are the same as for Numpy.</p> <li><p>Automatic differentiation is not only the chain rule&#33; Backpropagation algorithm &#40;or dual numbers&#41; is a clever algorithm to implement automatic differentiation...</p> </ul> </blockquote> </details> <h3 id=session_3 ><a href="#session_3" class=header-anchor >ðŸŒ» Session 3</a></h3> <ul> <li><p><a href="https://dataflowr.github.io/website/modules/3-loss-functions-for-classification/">Module 3 - Loss function for classification</a> </p> <li><p><a href="https://dataflowr.github.io/website/modules/4-optimization-for-deep-learning/">Module 4 - Optimization for deep learning</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/5-stacking-layers/">Module 5 - Stacking layers</a> and overfitting a MLP on CIFAR10</p> <li><p><a href="https://dataflowr.github.io/website/modules/6-convolutional-neural-network/">Module 6: Convolutional neural network</a></p> <li><p>how to regularize with dropout and uncertainty estimation with MC Dropout: <a href="https://dataflowr.github.io/website/modules/15-dropout/">Module 15 - Dropout</a></p> </ul> <details> <summary>Things to remember</summary> <blockquote> <ul> <li><p>Loss vs Accuracy. Know your loss for a classification task&#33;</p> <li><p>know your optimizer &#40;Module 4&#41;</p> <li><p>know how to build a neural net with torch.nn.module &#40;Module 5&#41;</p> <li><p>know how to use convolution and pooling layers &#40;kernel, stride, padding&#41;</p> <li><p>know how to use dropout </p> </ul> </blockquote> </details> <h3 id=session_4 ><a href="#session_4" class=header-anchor >ðŸŒ» Session 4</a></h3> <ul> <li><p><a href="https://dataflowr.github.io/website/modules/7-dataloading/">Module 7 - Dataloading</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/8a-embedding-layers/">Module 8a - Embedding layers</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/8b-collaborative-filtering/">Module 8b - Collaborative filtering</a> and build your own recommender system: <a href="https://github.com/dataflowr/notebooks/blob/master/Module8/08_collaborative_filtering_empty.ipynb">08&#95;collaborative&#95;filtering&#95;empty.ipynb</a> &#40;on a larger dataset <a href="https://github.com/dataflowr/notebooks/blob/master/Module8/08_collaborative_filtering_1M.ipynb">08&#95;collaborative&#95;filtering&#95;1M.ipynb</a>&#41;</p> <li><p><a href="https://dataflowr.github.io/website/modules/8c-word2vec/">Module 8c - Word2vec</a> and build your own word embedding <a href="https://github.com/dataflowr/notebooks/blob/master/Module8/08_Word2vec_pytorch_empty.ipynb">08&#95;Word2vec&#95;pytorch&#95;empty.ipynb</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/16-batchnorm/">Module 16 - Batchnorm</a> and check your understanding with <a href="https://github.com/dataflowr/notebooks/blob/master/Module16/16_simple_batchnorm_eval.ipynb">16&#95;simple&#95;batchnorm&#95;eval.ipynb</a> and more <a href="https://github.com/dataflowr/notebooks/blob/master/Module16/16_batchnorm_simple.ipynb">16&#95;batchnorm&#95;simple.ipynb</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/17-resnets/">Module 17 - Resnets</a></p> </ul> <details> <summary>Things to remember</summary> <blockquote> <ul> <li><p>know how to use dataloader</p> <li><p>to deal with categorical variables in deep learning, use embeddings</p> <li><p>in the case of word embedding, starting in an unsupervised setting, we built a supervised task &#40;i.e. predicting central / context words in a window&#41; and learned the representation thanks to negative sampling</p> <li><p>know your batchnorm</p> <li><p>architectures with skip connections allows deeper models</p> </ul> </blockquote> </details> <h3 id=session_5 ><a href="#session_5" class=header-anchor >ðŸŒ» Session 5</a></h3> <ul> <li><p><a href="https://dataflowr.github.io/website/modules/9a-autoencoders/">Module 9a: Autoencoders</a> and code your noisy autoencoder <a href="https://github.com/dataflowr/notebooks/blob/master/Module9/09_AE_NoisyAE.ipynb">09&#95;AE&#95;NoisyAE.ipynb</a></p> <li><p><a href="">Module 10: Generative Adversarial Networks</a> and code your GAN, Conditional GAN and InfoGAN <a href="https://github.com/dataflowr/notebooks/blob/master/Module10/10_GAN_double_moon.ipynb">10&#95;GAN&#95;double&#95;moon.ipynb</a></p> <li><p><a href="https://dataflowr.github.io/website/modules/13-siamese/">Module 13: Siamese Networks and Representation Learning</a></p> </ul> <h3 id=session_6 ><a href="#session_6" class=header-anchor >ðŸŒ» Session 6</a></h3> <p>TBA</p> <h2 id=curators ><a href="#curators" class=header-anchor >Curators</a></h2> <p><a href="https://www.di.ens.fr/~lelarge/">Marc Lelarge</a>, <a href="https://abursuc.github.io/">Andrei Bursuc</a> with <a href="https://jill-jenn.net/">Jill-JÃªnn Vie</a></p> <h2 id=course_in_a_hurry ><a href="#course_in_a_hurry" class=header-anchor >Course in a hurry</a></h2> <p><strong>Super fast track</strong> to learn the basics of deep learning from scratch:</p> <ul> <li><p>Have a look at the <a href="https://dataflowr.github.io/slides/module1.html">slides</a> of <a href="./modules/1-intro-general-overview">Module 1: Introduction &amp; General Overview</a></p> <li><p>Run the <a href="https://github.com/dataflowr/notebooks/blob/master/Module2/02a_basics.ipynb">notebook</a> &#40;or in <a href="https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module2/02a_basics.ipynb">colab</a>&#41; of <a href="./modules/2a-pytorch-tensors">Module 2a: Pytorch Tensors</a></p> <li><p>Run the <a href="https://github.com/dataflowr/notebooks/blob/master/Module2/02b_linear_reg.ipynb">notebook</a> &#40;or in <a href="https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module2/02b_linear_reg.ipynb">colab</a>&#41; of <a href="./modules/2b-automatic-differentiation">Module 2b: Automatic Differentiation</a></p> <li><p>Check the <a href="./modules/3-loss-functions-for-classification/#minimal_working_examples">Minimal working examples</a> of <a href="./modules/3-loss-functions-for-classification">Module 3: Loss functions for classification</a>. If you do not understand, have a look at the <a href="https://dataflowr.github.io/slides/module3.html">slides</a>.</p> <li><p>Have a look at the <a href="https://dataflowr.github.io/slides/module4.html">slides</a> of <a href="./modules/4-optimization-for-deep-learning">Module 4: Optimization for Deep Learning</a></p> <li><p>Try playback speed 1.5 for the <a href="https://youtu.be/OiyZXdnLHcI?t&#61;149">video</a> from <a href="./modules/5-stacking-layers">Module 5: Stacking layers</a>.</p> <li><p>Run the <a href="https://github.com/dataflowr/notebooks/blob/master/Module6/06_convolution_digit_recognizer.ipynb">notebook</a> &#40;or in <a href="https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module6/06_convolution_digit_recognizer.ipynb">colab</a>&#41; of <a href="./modules/6-convolutional-neural-network">Module 6: Convolutional Neural Network</a></p> <li><p>Try playback speed 2 for the <a href="https://youtu.be/vm-ZusIUkiY?t&#61;133">video</a> from <a href="./modules/7-dataloading">Module 7: Dataloading</a></p> <li><p>Have a look at the <a href="https://dataflowr.github.io/slides/module8a.html">slides</a> of <a href="./modules/8a-embedding-layers">Module 8a: Embedding layers</a></p> <li><p>Well done&#33; Now you have time to enjoy deep learning&#33;</p> </ul> <h2 id=for_contributors ><a href="#for_contributors" class=header-anchor >For contributors</a></h2> <p>Join the <a href="https://github.com/dataflowr">GitHub repo dataflowr</a> and make a pull request. <a href="https://yangsu.github.io/pull-request-tutorial/">What are pull requests?</a></p> <p>Thanks to <a href="https://github.com/dhuynh95">Daniel Huynh</a>, <a href="https://github.com/ericdaat">Eric Daoud</a>, <a href="https://github.com/SimonCoste">Simon Coste</a></p> <p>Materials from this site is used for courses at ENS and X. </p> <img src="/website/assets/ENS_logo.jpg" style="width: 400px; height: auto; display: inline"> <img src="/website/assets/X_logo.png" style="margin-left:1em; width: 180px; height: auto; display: inline"> <div class=page-foot > <div class=copyright > <a href="https://github.com/dataflowr/website/tree/master"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> Last modified: May 16, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div>