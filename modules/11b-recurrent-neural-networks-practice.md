@def sequence = ["rnn-2"]

# Module 11b - Recurrent Neural Networks practice


**Table of Contents**

\toc


## Theory of RNNs

{{youtube_placeholder rnn-2}}
{{yt_tsp 0 0 Generating the dataset for binary classification of parentheses}}
{{yt_tsp 296 0 Elman network}}
{{yt_tsp 685 0 RNN with gating}}
{{yt_tsp 846 0 LSTM}}
{{yt_tsp 1113 0 Be careful with errors given on the training set!}}

## Notebook

- [notebook](https://github.com/dataflowr/notebooks/blob/master/Module11/11_RNN.ipynb) in [colab](https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module11/11_RNN.ipynb)

## Practicals

- [notebook](https://github.com/dataflowr/notebooks/blob/master/Module11/11_predicitions_RNN_empty.ipynb) (or opened in [colab](https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module11/11_predicitions_RNN_empty.ipynb)) for predicting engine failure with RNN.

## References

> RNNs can generate bounded hierarchical languages with optimal memory (2020) John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, Christopher D. Manning  [arXiv:2010.07515](https://arxiv.org/abs/2010.07515)

> Self-Attention Networks Can Process Bounded Hierarchical Languages (2021) Shunyu Yao, Binghui Peng, Christos Papadimitriou, Karthik Narasimhan [arXiv:2105.11115](https://arxiv.org/abs/2105.11115)