<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/website/libs/highlight/github.min.css"> <link rel=stylesheet  href="/website/css/franklin.css"> <link rel=stylesheet  href="/website/css/poole_hyde.css"> <link rel=stylesheet  href="/website/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/website/assets/favicon.png"> <title>Dataflowr - Deep Learning DIY</title> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/website/assets/dataflowr_violet_plain_square.png" style="width: 120px; height: auto; display: inline"> <img src="/website/assets/favicon.png" style="margin-left:1em; position:relative;left:0px; top:-30px; width: 60px; height: auto; display: inline"> <h1 style="font-size:1em; opacity: 0.95;"><a href="/website/">Deep Learning DIY</a></h1> </div> <nav class=sidebar-nav > <a class="sidebar-nav-item " href="/website/modules/0-sotfware-installation"> <b>Module 0</b> - <em> Software installation</em> </a> <a class="sidebar-nav-item " href="/website/modules/1-intro-general-overview"> <b>Module 1</b> - <em>Introduction & General Overview</em> </a> <a class="sidebar-nav-item " href="/website/modules/2a-pytorch-tensors"> <b>Module 2a</b> - <em>PyTorch tensors</em> </a> <a class="sidebar-nav-item " href="/website/modules/2b-automatic-differentiation"> <b>Module 2b</b> - <em>Automatic differentiation</em> </a> <a class="sidebar-nav-item " href="/website/modules/2c-jax"> <b>Module 2c</b> - <em>Automatic differentiation: VJP and intro to JAX</em> </a> <a class="sidebar-nav-item active" href="/website/modules/3-loss-functions-for-classification"> <b>Module 3</b> - <em>Loss functions for classification</em> </a> <a class="sidebar-nav-item " href="/website/modules/4-optimization-for-deep-learning"> <b>Module 4</b> - <em>Optimization for DL</em> </a> <a class="sidebar-nav-item " href="/website/modules/5-stacking-layers"> <b>Module 5</b> - <em>Stacking layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/6-convolutional-neural-network"> <b>Module 6</b> - <em>Convolutional neural network</em> </a> <a class="sidebar-nav-item " href="/website/modules/7-dataloading"> <b>Module 7</b> - <em>Dataloading</em> </a> <a class="sidebar-nav-item " href="/website/modules/8a-embedding-layers"> <b>Module 8a</b> - <em>Embedding layers</em> </a> <a class="sidebar-nav-item " href="/website/modules/8b-collaborative-filtering"> <b>Module 8b</b> - <em>Collaborative filtering</em> </a> <a class="sidebar-nav-item " href="/website/modules/8c-word2vec"> <b>Module 8c</b> - <em>Word2vec</em> </a> <a class="sidebar-nav-item " href="/website/modules/9a-autoencoders"> <b>Module 9a</b> - <em>Autoencoders</em> </a> <a class="sidebar-nav-item " href="/website/modules/9b-unet"> <b>Module 9b</b> - <em>UNets</em> </a> <a class="sidebar-nav-item " href="/website/modules/9c-flows"> <b>Module 9c</b> - <em>Flows</em> </a> <a class="sidebar-nav-item " href="/website/modules/10-generative-adversarial-networks"> <b>Module 10</b> - <em>Generative adversarial networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/11a-recurrent-neural-networks-theory"> <b>Module 11a</b> - <em>Recurrent Neural Networks (theory)</em> </a> <a class="sidebar-nav-item " href="/website/modules/11b-recurrent-neural-networks-practice"> <b>Module 11b</b> - <em>RNN in practice</em> </a> <a class="sidebar-nav-item " href="/website/modules/11c-batches-with-sequences"> <b>Module 11c</b> - <em>Batches with sequences in Pytorch</em> </a> <a class="sidebar-nav-item " href="/website/modules/12-attention"> <b>Module 12</b> - <em>Attention and Transformers</em> </a> <a class="sidebar-nav-item " href="/website/modules/13-siamese"> <b>Module 13</b> - <em>Siamese Networks and Representation Learning</em> </a> <a class="sidebar-nav-item " href="/website/modules/14a-depth"> <b>Module 14a</b> - <em>The Benefits of Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/14b-depth"> <b>Module 14b</b> - <em>The Problems with Depth</em> </a> <a class="sidebar-nav-item " href="/website/modules/15-dropout"> <b>Module 15</b> - <em>Dropout</em> </a> <a class="sidebar-nav-item " href="/website/modules/16-batchnorm"> <b>Module 16</b> - <em>Batchnorm</em> </a> <a class="sidebar-nav-item " href="/website/modules/17-resnets"> <b>Module 17</b> - <em>Resnets</em> </a> <a class="sidebar-nav-item " href="/website/modules/18a-diffusion"> <b>Module 18a</b> - <em>Denoising Diffusion Probabilistic Models</em> </a> <!-- <div class=week >Unit 7</div>--> <div class=week >Homeworks</div> <a class="sidebar-nav-item " href="/website/homework/1-mlp-from-scratch"> <b>Homework 1</b> - <em>MLP from scratch</em> </a> <a class="sidebar-nav-item " href="/website/homework/2-CAM-adversarial"> <b>Homework 2</b> - <em>Class Activation Map and adversarial examples</em> </a> <a class="sidebar-nav-item " href="/website/homework/3-VAE"> <b>Homework 3</b> - <em>VAE for MNIST clustering and generation</em> </a> <div class=week >Bonus</div> <a class="sidebar-nav-item " href="/website/modules/12-intro-julia"> <b>Module</b> - <em>Intro to Julia: Autodiff with dual numbers</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph0"> <b>Module</b> - <em>Deep learning on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph1"> <b>Graph</b> - <em>Node embeddings</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph2"> <b>Graph</b> - <em>Signal processing on graphs</em> </a> <a class="sidebar-nav-item " href="/website/modules/graph3"> <b>Graph</b> - <em> Graph embeddings and GNNs</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/GCN_inductivebias_spectral"> <b>Post</b> - <em>Spectral GCN</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/Convolutions_first"> <b>Post</b> - <em>Convolutions from first principles</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/invariant_equivariant"> <b>Post</b> - <em>Invariant and equivariant networks</em> </a> <a class="sidebar-nav-item " href="/website/modules/extras/graph_invariant"> <b>Graph</b> - <em>Exploiting Graph Invariants in Deep Learning</em> </a> <div class=week >Guest Lectures</div> <a class="sidebar-nav-item " href="/website/modules/privacy-preserving-ML"> <b>Privacy Preserving ML</b> - <em>Daniel Huynh</em> </a> </nav> </div> </div> <div class="content container"> <div class=franklin-content ><h1 id=module_3_-_loss_functions_for_classification ><a href="#module_3_-_loss_functions_for_classification" class=header-anchor >Module 3 - Loss functions for classification</a></h1> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#loss_functions_for_classification">Loss functions for classification</a><li><a href="#slides_and_notebook">Slides and Notebook</a><li><a href="#minimal_working_examples">Minimal working examples</a><ol><li><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss"><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code>BCELoss</code></a></a><li><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss"><a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"><code>NLLLoss</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"><code>CrossEntropyLoss</code></a></a></ol><li><a href="#quiz">Quiz</a></ol></div> <h2 id=loss_functions_for_classification ><a href="#loss_functions_for_classification" class=header-anchor >Loss functions for classification</a></h2> <div id=videoContainer  > <div id=player ></div> </div> <script> var tag = document.createElement('script'); tag.src = "https://www.youtube.com/iframe_api"; var firstScriptTag = document.getElementsByTagName('script')[0]; firstScriptTag.parentNode.insertBefore(tag, firstScriptTag); var player; function onYouTubeIframeAPIReady() { player = new YT.Player('player', { height: '300', width: '100%', videoId: 'jReGEZXq4Ac', playerVars: { 'autoplay': 0, 'rel': 0, 'cc_load_policy': 1 } }); } function changeYouTubeSource(startTime, endTime) { var youtubeIframe = document.getElementById('player'); var youtubeIframeSrc = document.getElementById('player').getAttribute('src'); var trimmedIframeUrl = ''; var iframeUrlTimeStamp = ''; if (youtubeIframeSrc.match(/&start=/g)) { var mediaFragmentIndex = youtubeIframeSrc.indexOf('&start='); trimmedIframeUrl = youtubeIframeSrc.slice(0, mediaFragmentIndex); if (endTime === 0) { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime; } else { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime + '&end=' + endTime; } } if (youtubeIframeSrc.match(/&start=/g) === null) { if (endTime === 0) { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime; } else { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime + '&end=' + endTime; } } setTimeout(function() { var iframeAutoplayUrl = iframeUrlTimeStamp.replace('autoplay=0', 'autoplay=1'); youtubeIframe.setAttribute('src', iframeAutoplayUrl); }, 1000); } </script> <p><br> <a href='#player' onclick='changeYouTubeSource(0,0)'> 0:00</a> Recap <br> <a href='#player' onclick='changeYouTubeSource(145,0)'> 2:25</a> How to choose your loss? <br> <a href='#player' onclick='changeYouTubeSource(198,0)'> 3:18</a> A probabilistic model for linear regression <br> <a href='#player' onclick='changeYouTubeSource(470,0)'> 7:50</a> Gradient descent, learning rate, SGD <br> <a href='#player' onclick='changeYouTubeSource(690,0)'> 11:30</a> Pytorch code for gradient descent <br> <a href='#player' onclick='changeYouTubeSource(915,0)'> 15:15</a> A probabilistic model for logistic regression <br> <a href='#player' onclick='changeYouTubeSource(1047,0)'> 17:27</a> Notations (information theory) <br> <a href='#player' onclick='changeYouTubeSource(1258,0)'> 20:58</a> Likelihood for logistic regression <br> <a href='#player' onclick='changeYouTubeSource(1363,0)'> 22:43</a> BCELoss <br> <a href='#player' onclick='changeYouTubeSource(1421,0)'> 23:41</a> BCEWithLogitsLoss <br> <a href='#player' onclick='changeYouTubeSource(1537,0)'> 25:37</a> Beware of the reduction parameter <br> <a href='#player' onclick='changeYouTubeSource(1647,0)'> 27:27</a> Softmax regression <br> <a href='#player' onclick='changeYouTubeSource(1852,0)'> 30:52</a> NLLLoss <br> <a href='#player' onclick='changeYouTubeSource(2088,0)'> 34:48</a> Classification in pytorch <br> <a href='#player' onclick='changeYouTubeSource(2196,0)'> 36:36</a> Why maximizing accuracy directly is hard? <br> <a href='#player' onclick='changeYouTubeSource(2304,0)'> 38:24</a> Classification in deep learning <br> <a href='#player' onclick='changeYouTubeSource(2450,0)'> 40:50</a> Regression without knowing the underlying model <br> <a href='#player' onclick='changeYouTubeSource(2578,0)'> 42:58</a> Overfitting in polynomial regression <br> <a href='#player' onclick='changeYouTubeSource(2720,0)'> 45:20</a> Validation set <br> <a href='#player' onclick='changeYouTubeSource(2935,0)'> 48:55</a> Notion of risk and hypothesis space <br> <a href='#player' onclick='changeYouTubeSource(3280,0)'> 54:40</a> estimation error and approximation error </p> <h2 id=slides_and_notebook ><a href="#slides_and_notebook" class=header-anchor >Slides and Notebook</a></h2> <ul> <li><p><a href="https://dataflowr.github.io/slides/module3.html">slides</a></p> <li><p><a href="https://github.com/dataflowr/notebooks/blob/master/Module3/03_polynomial_regression.ipynb">notebook</a> in <a href="https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module3/03_polynomial_regression.ipynb">colab</a> An explanation of underfitting and overfitting with polynomial regression.</p> </ul> <h2 id=minimal_working_examples ><a href="#minimal_working_examples" class=header-anchor >Minimal working examples</a></h2> <h3 id=a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss ><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss" class=header-anchor ><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code>BCELoss</code></a></a></h3> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch.nn <span class=hljs-keyword >as</span> nn
m = nn.Sigmoid()
loss = nn.BCELoss()
<span class=hljs-built_in >input</span> = torch.randn(<span class=hljs-number >3</span>,<span class=hljs-number >4</span>,<span class=hljs-number >5</span>)
target = torch.randn(<span class=hljs-number >3</span>,<span class=hljs-number >4</span>,<span class=hljs-number >5</span>)
loss(m(<span class=hljs-built_in >input</span>), target)</code></pre> <h3 id=a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss ><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss" class=header-anchor ><a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"><code>NLLLoss</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"><code>CrossEntropyLoss</code></a></a></h3> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch.nn <span class=hljs-keyword >as</span> nn
m = nn.LogSoftmax(dim=<span class=hljs-number >1</span>)
loss1 = nn.NLLLoss()
loss2 = nn.CrossEntropyLoss()
C = <span class=hljs-number >8</span>
<span class=hljs-built_in >input</span> = torch.randn(<span class=hljs-number >3</span>,C,<span class=hljs-number >4</span>,<span class=hljs-number >5</span>)
target = torch.empty(<span class=hljs-number >3</span>,<span class=hljs-number >4</span>,<span class=hljs-number >5</span> dtype=torch.long).random_(<span class=hljs-number >0</span>,C) 
<span class=hljs-keyword >assert</span> loss1(m(<span class=hljs-built_in >input</span>),target) == loss2(<span class=hljs-built_in >input</span>,target)</code></pre> <h2 id=quiz ><a href="#quiz" class=header-anchor >Quiz</a></h2> <p>To check you know your loss, you can do the <a href="https://dataflowr.github.io/quiz/module3.html">quizzes</a></p> <div class=page-foot > <div class=copyright > <a href="https://github.com/dataflowr/website/tree/master"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> Last modified: May 01, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div>